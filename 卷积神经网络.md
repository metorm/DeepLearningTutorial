# 概念

## 卷积核

一组（权重+偏置）的数值，这些数值常以方阵的形式组织，例如5x5，用于从一块5x5的图像中提取某种特征，此时这个卷积核具有5x5+1=26个参数。具体的作用方式或者说输出为：

$$\sigma\left(b + \sum_{l=0}^4 \sum_{m=0}^4  w_{l,m} a_{j+l, k+m} \right)$$

实质上就是用一个标量表达了该小区域内的图像与目标特征的匹配程度。所以卷积核也可以说是某种过滤器。

### 共享权重/偏置
其实就是指，同一个卷积核作用于不同的区域，其权重/偏置不变。从神经元连接的角度，这可以看作是下一层这些神经元使用了相同的权重和偏置。

## 通道

每个卷积核的输入范围在原始图像（矩阵）上滑动，可以生成一个略小一些的矩阵，这称为一个通道。每个卷积核对应于一个通道。实用的神经网络可以有很多（几十个）通道。

## 池化 Pooling

一个卷积层背后的输出，直观意义是这个小区域内图像与某特征的匹配程度。由于卷积是一个像素一个像素滑动的，则两个相邻的输出，其源数据仅仅是在一个方向上差了一个像素，本身应该也相差不大，可以再次简化。

Pooling（池化）操作，就是将相邻的一个小区域（例如2x2）内的卷积输出，通过取平均值或者最大值，简化为一个数值。这里，操作区域不像卷积是滑动的，而是一块一块平铺的。例如，28x28的原始图像，经过5x5大小的卷积，输出为(28-5+1)x(28-5+1) = 24x24。再经过2x2的池化，输出为(24/2)x(24/2) = 12x12.

取最大值的池化，又可以直观地解释为：如果某个区域内某个通道上匹配度很高，那么后续步骤中知道这附近有这个特征即可，精确的位置信息不是十分重要。

池化是深度学习中主要的降维操作。

# 反向传播算法

直观上看，卷积神经网络的行为与普通神经网络不同。但是实际上，卷积与池化，也可以看作是神经元之间特殊的权重与偏置关系：卷积范围之外，或者池化过程中被忽略掉的部分，可以认为是权重为零的连接。

故而，这些层之间的反向传播，也可以由前面的基本方程来描述。但是，由于连接的特殊性，实际上有更快的特殊算法来计算卷积、池化层之间的反向传播。

## 池化层反向传播到卷积层

以2x2池化为例。

池化层中一个神经元对应于前面卷积层中的四个。若为取最大值池化，由基本方程2，可知反向传播的残差也全部传播到上一层四个神经元中激活函数最大的那一个（这个连接上权重为1），其余均为零。若为平均值池化，则显然四个节点的权重全是0.25，平分为四份反向传播回去即可。

## 卷积层反向传播到池化层

这是网络中存在多个（卷积/池化）层对的情况。实质上这个问题跟前一层是池化层没有直接关系。问题的本质在于找出卷积过程中，相对于卷积结果有权重的神经元。

先上结论：已知卷积层上每个神经元的$\delta$，为了计算在某个卷积变换后上一层的$\delta$，可以将卷积核旋转180度，随后使用旋转后的卷积核对后一层的$\delta$矩阵进行“full”模式卷积运算（边缘补0），得到的结果即为前一层网络上应有的$\delta$矩阵。

证明：

基本方程2：某一层第i个节点的残差等于与该节点有权值连接的后一层所有节点上的残差乘以连接权重的和，最后和乘以激活函数在该节点输入值上的导数。

设3x3卷积核的9个数值分别为：

$$w_{11}, w_{12}, w_{13}$$
$$w_{21}, w_{22}, w_{23}$$
$$w_{31}, w_{32}, w_{33}$$

对于前一层上某个坐标为$i,j$神经元，下一层中最多有9个神经元受到其影响，分布在它对应位置周围。设这9个受影响的神经元为：

$$A_{11}, A_{12}, A_{13}$$
$$A_{21}, A_{22}, A_{23}$$
$$A_{31}, A_{32}, A_{33}$$

分析卷积过程可以发现，神经元$i,j$影响$A_{11}$的方式是通过以$i-1,j-1$为中心的那一次卷积运算，在这次卷积运算中，$i,j$位于卷积核的右下角，对应的权重是$w_{33}$。

类似的，该神经元以权重$w_{32}$影响$A_{12}$，以权重$w_{31}$影响$A_{13}$，以权重$w_{23}$影响$A_{21}$……逐个分析并综合，这些权重的排列方式恰好是原卷积核旋转180度：

$$w_{33}, w_{32}, w_{31}$$
$$w_{23}, w_{22}, w_{21}$$
$$w_{13}, w_{12}, w_{11}$$

如此，直接以旋转后的卷积核在后一层的$\delta$矩阵上做卷积，即可得到前一层的$\delta$矩阵。对于边界上的神经元，需要在后一层矩阵周围补0，参考“full”模式的卷积。该模式下，卷积后矩阵尺寸变大。