# 平方代价函数之缺点

参考前面的笔记“反向传播算法（BP）4个基本方程之推导”，可以发现，$delta$函数是一个很基础的中间量，它出现在每个神经元的权重/偏置的梯度计算公式中作为因子。

问题在于，根据反向传播基本方程1，$\delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j)$,该公式中的最后一项$\sigma'$有一讨厌的特性：在$\sigma$的输出接近于0或者1时，$\sigma'$的绝对值非常小，相应地造成了根据公式计算出来的学习量（梯度x学习率）也相对于$\sigma \approx 0.5 $的时候很小，而不管此时神经网络训练是否已经收敛。这将减慢网络的训练速度。同时，这也与“误差越大，学习越快”的直觉不符。

在理论框架确定的情况下，四个基本方程已经确定，无法避免公式中$\sigma'$项的出现。一条可能的优化路径是：通过构造一种代价函数（不再使用平方代价函数$C=\frac{1}{2} \sum_{j}{\left( y_i - a_i \right)^2}$），使得 $\delta$ 计算公式中前一项 $\frac{\partial C}{\partial a^L_j}$ 中出现 $\sigma'$ 的倒数，即可从源头消去 $\sigma'$，加速神经元接近饱和时的学习速度。

# 交叉熵代价函数

交叉熵代价函数即符合这个要求。考虑单个神经元，交叉熵代价函数定义为：
$$ C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right] $$
式中$x$为神经元上的所有输入量（$z = wx +b$），也就是上一层的$a$。$y$ 为期望的输出。$a$ 为本层神经元的激活输出。

这一函数形式复杂，应该是某大神精心构造而来。该函数恒为非负，在$y=a$时取得最小值(如果此时$y$等于0或者1，$C$应该很接近于0。$y$等于0或者1多出现在分类问题中。)。这些特征说明本函数适合作为代价函数使用。

更具体的特征可以画线出来看。详细研究本函数的性质就太麻烦了。

## 证明

直接对$C$取$a$的偏导数，有：
$$\frac{\partial C}{\partial a} = \frac{y}{a} - \frac{1-y}{1-a} = \frac{y-a}{a(1-a)}$$

对$\sigma(z)$求导，有：
$$\sigma'(z) = \sigma(z)(1-\sigma(z)) = a(1-a)$$

代入$\delta$的计算式，有：
$$\delta = \frac{\partial C}{\partial a} \sigma'(z) = \frac{y-a}{a(1-a)} a(1-a) = y-a$$

# 总结

可以看到，引入交叉熵函数之后，对神经网络训练过程的改动其实非常之小：仅仅是略去了由期望输出计算最后一层的$delta$这一计算步骤中的$\sigma'$，其余计算步骤相对于平方代价函数基本上保持不变，仍是通过后面另外那三个基本方程迭代计算、反向传播。